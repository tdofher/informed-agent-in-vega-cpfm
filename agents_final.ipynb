{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"agents_final.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOj5zK48pTT5oY/gU98pdpl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"M8fYHnyrYCBJ"},"source":["# Actor Critic class\n","\n","Refereneces:\n","\n","*   https://pytorch.org/docs/stable/generated/torch.where.html#torch.where\n","* https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax\n","* https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam\n","* https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR\n","* https://pytorch.org/docs/stable/distributions.html#categorical\n","* https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html#torch.nn.functional.one_hot\n","* https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave\n","\n"]},{"cell_type":"markdown","source":["https://peps.python.org/pep-0008/"],"metadata":{"id":"k_kL56pM7Ql2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ooy922JpBYxc"},"outputs":[],"source":["class CPFMAgent():\n","  \"\"\" \n","  This class represents an agent trading in a Constant Product Market for \n","  cash settled futures as outlined in the paper:\n","  \"Constant Product Market for Cash Settled Futures\" by W. Gawlikowicz,\n","   B. Mannerings and D. Siska\n","\n","  Contains all the state updates for an agent trading in this market including\n","  price updates, as defined in the above paper\n","  \"\"\"\n","  \n","  def __init__(self, utility, trade_size, fee_infrastructure, R_long,\n","               R_short, l_search, l_initial, l_release, random_agent_update,\n","               variable_fees, CPFM_kw_args, verbose = False, device = 'cpu',\n","               **kwargs):\n","    \"\"\"\n","    Initialize the CPFM market, setting all fixed parameters.\n","\n","    Parameters\n","    ----------\n","    utility : partial\n","      the utility function used to train an agent on this market \n","      (only relevant for trained agents)\n","    trade_size : float\n","      the minimum trade size for a futures trade on this market\n","    fee_infrastructure : float\n","      the fixed infrastructure fee for trades on this market\n","    R_long : float\n","      the long risk factor for the market, constant in this case\n","    R_short : float\n","      the short risk factor for the market, constant in this case\n","    l_search : float\n","      search margin scaling level (> 1)\n","    l_initial : float\n","      initial margin scaling level (> l_search)\n","    l_release : float \n","      release margin scaling level (> l_initial)\n","    random_agent_upate : bool\n","      whether or not the price update should use random trader agents, if \n","      False use the Price Match Update model. If true require that\n","      CPFM_kw_args contains keys 'num_traders', an int indicating the \n","      number of traders to simulate, and 'traders_pay_fees', a bool\n","      indicating whether or not the trader agents pay fees\n","    variable_fees : bool\n","      whether or not fees are variable in the market, if False require that\n","      CPFM_kw_args contains key 'fee_liquidity', a float setting the fixed \n","      liquidity fee\n","    CPFM_kw_args : dict\n","      contains keyword arguments necessary under certain condition\n","    verbose : bool, optional\n","      whether or not to print certain debugging messages. Defaults to False\n","    device : str, optional\n","      the device on which to run pytorch operations. Defaults to 'cpu'\n","    \"\"\"\n","   \n","    self.verbose = verbose\n","    self.device = device\n","    # Running cost / utility function\n","    self.f = utility \n","    # Set fees in the market\n","    self.variable_fees = variable_fees\n","    if not variable_fees: # Set the fixed liquidty fee\n","      self.f_L = CPFM_kw_args['fee_liquidity']\n","    self.f_I = fee_infrastructure  \n","    # Trade size\n","    self.ts = trade_size\n","    # Risk parameters\n","    self.R_long = R_long \n","    self.R_short = R_short\n","    self.l_s = l_search \n","    self.l_i = l_initial\n","    self.l_r = l_release\n","    # Set the price update model\n","    self.random_agent_update = random_agent_update\n","    if self.random_agent_update:\n","      # Set necessary parameters for random agent update model\n","      self.num_traders = CPFM_kw_args['num_traders']\n","      self.traders_pay_fees = CPFM_kw_args['traders_pay_fees']\n","      \n","  def M_long(self, A_B, A_S):\n","    \"\"\"Get maintenance margin for long position\"\"\"\n","    return A_B / A_S * self.R_long\n","\n","  def M_short(self, A_B, A_S):\n","    \"\"\"Get maintenance margin for short position\"\"\"\n","    return A_B / A_S * self.R_short\n","\n","  def pool_ratio_deltas(self, delta, A_B, A_S):\n","    \"\"\"\n","    Obtain changes to A_B and A_S needed to adjust the total pool balance by \n","    amount delta without changing the ratio\n","    \"\"\"\n","    deltaS = delta * A_S / (A_B+A_S)\n","    deltaB = delta * A_B / (A_B+A_S)\n","    return deltaB, deltaS\n","\n","  def maintain_pool_ratio(self, delta, A_B, A_S):\n","    \"\"\"\n","    Update total pool balance A_B + A_S by delta without changing the ratio\n","    \"\"\"\n","    deltaS, deltaB = self.pool_ratio_deltas(delta, A_B, A_S)\n","    A_S = A_S + deltaS\n","    A_B = A_B + deltaB\n","    return A_B, A_S\n","  \n","  def check_pool_price_update(self, A_B, A_S, dn):\n","    \"\"\"Ensure that pool price update will not cause distressed pool\"\"\"\n","    P_old = A_B / A_S\n","    return torch.logical_and(A_B + dn*self.ts*P_old > 0,\n","                             A_S - dn*self.ts*P_old > 0)\n","\n","  def pool_price_update(self, A_B, A_S, dn):\n","    \"\"\" \n","    Adjust the pool position (and therefore the price) based on a trade of\n","    dn * trade size\n","    \"\"\"\n","    P_old = A_B / A_S\n","    A_S = A_S - dn*self.ts*P_old\n","    A_B = A_B + dn*self.ts*P_old\n","    return A_B, A_S\n","\n","  def margin(self, A_B, A_S, n, g, m, f_L, dn, close_out=False):\n","    \"\"\"\n","    Update agent's positions assuming they meet margin and fee requirements\n","    Coresponds to sections 2.1.1 - 2.1.5 in the paper \"Constant Product Market\n","    for Cash Settled Futures\"\n","\n","    dn : float\n","        change in position in terms of fraction of trade size, in [-1, 1]\n","    close_out : bool, optional\n","        whether or not transaction is a close out trade. Defaults to False\n","    \"\"\"\n","\n","    if type(dn) is int:\n","      dn = torch.ones_like(A_B, device=self.device) * dn\n","    # Make necessary adjustments for long or short position\n","    M = torch.where(n + dn >= 0, self.M_long(A_B, A_S), self.M_short(A_B, A_S))\n","    # Fees\n","    if close_out: \n","      # Do not charge fees on close out trades\n","      f_L_loc = torch.zeros_like(dn, device=self.device)\n","      F_L = torch.zeros_like(dn, device=self.device)\n","    else:\n","      f_L_loc = f_L.clone()\n","      F_L = torch.abs(dn) * self.ts * A_B / A_S * f_L    \n","    f_I = self.f_I if not close_out else 0.       \n","    F_I = torch.abs(dn) * self.ts * A_B / A_S * f_I\n","    F = F_L + F_I\n","    # Margin required for position\n","    margin_req = torch.abs(n+dn) * self.ts * M * self.l_i \n","    # Whether or not user is increasing their position\n","    is_increasing = torch.abs(n+dn) >= torch.abs(n)  \n","    if close_out:\n","      # Always allow the user to trade if closing out a position\n","      can_trade = torch.ones_like(A_B, device=self.device).to(torch.bool)\n","    else:\n","      # If reducing position, user can trade if fees can be paid\n","      # from general account. If increasing position, user can trade\n","      # if fees can be paid from general account and the required \n","      # margin an be met from general and margin account   \n","      can_trade = (g >= F) & (~is_increasing | (g + m >= F + margin_req)) \n","      # Ensure trade will not make either of the pools <= 0 \n","      can_trade = can_trade & self.check_pool_price_update(A_B, A_S, dn)\n","    b = torch.where(is_increasing & can_trade & (margin_req > m),\n","                    margin_req - m, 0.)\n","    n = torch.where(can_trade, n + dn, n) \n","    # Only charge fee if a trade occurred, and fee not already 0\n","    F = torch.where(can_trade & ((f_L_loc + f_I) > 0.), F, 0.)\n","    F_L = torch.where(can_trade & ((f_L_loc + f_I) > 0.),\n","                      F * f_L / (f_L+f_I), 0.)\n","    F_I = torch.where(can_trade & ((f_L_loc + f_I) > 0.),\n","                      F * f_I / (f_L+f_I), 0.) \n","    g = g - F - b \n","    m = m + b\n","    # Adjust the pool position based on the trade\n","    A_B[can_trade], A_S[can_trade] =\\\n","        self.pool_price_update(A_B[can_trade], A_S[can_trade], dn[can_trade])\n","    # Add fees to pools\n","    A_B, A_S = self.maintain_pool_ratio(F_L, A_B, A_S) \n","    return A_B, A_S, n, g, m\n","\n","  def mark_to_market(self, P_old, A_B, A_S, n, g, m):\n","      \"\"\" \n","      Mark-to-market according to 2.1.6 in the paper \"Constant Product Market\n","      for Cash Settled Futures\" \n","      \"\"\"\n","\n","      deltaP = (A_B / A_S) - P_old\n","      # For now assume infite margin\n","      delta = torch.where(n * self.ts * deltaP + m > 0,\n","                          n * self.ts * deltaP, -m)                   \n","      deltaB, deltaS = self.pool_ratio_deltas(-delta, A_B, A_S)\n","      # Check if pool distressed, if so do not mark to market. \n","      # Withdraw margin to general account and set position to 0\n","      distressed = (-deltaB > A_B) | (-deltaS > A_S)\n","      if torch.any(distressed):\n","        print('Distressed pool')\n","      A_B = torch.where(distressed, A_B, A_B + deltaB)\n","      A_S = torch.where(distressed, A_S, A_S + deltaS)\n","      n = torch.where(distressed, 0., n)\n","      g = torch.where(distressed, g + m, g)\n","      m = torch.where(distressed, 0., m + delta)   \n","      return A_B, A_S, n, g, m\n","\n","  def margin_checks(self, A_B, A_S, n, g, m, f_L):\n","    \"\"\"\n","    Perform margin checks according to 2.1.7 in the paper \"Constant Product \n","    Market for Cash Settled Futures\"\n","    \"\"\"\n","\n","    M = torch.where(n >= 0, self.M_long(A_B, A_S), self.M_short(A_B, A_S))\n","    # Top up margin from general account if needed \n","    b = torch.maximum(torch.minimum(torch.abs(n) * self.ts * M * self.l_s - m,\n","                                    g), torch.zeros_like(n))\n","    m = m + b\n","    g = g - b\n","    # Check if agent distressed: Steps 3 - 4\n","    is_dis = ~((n == 0.) | (m > torch.abs(n) * self.ts * M))\n","    if torch.any(is_dis):\n","      #print('Distressed')\n","      # Submit close out trades until agent not distressed: Step 5  \n","      dn = torch.where(torch.abs(n[is_dis]) > 1.,\n","                       torch.where(n[is_dis] > 0., -1., 1.).to(torch.float64),\n","                       -n[is_dis])\n","      n_old = n[is_dis]  \n","      # Note: Causes recursion  \n","      A_B[is_dis], A_S[is_dis], n[is_dis], g[is_dis], m[is_dis] =\\\n","        self.make_trade(A_B[is_dis], A_S[is_dis], n[is_dis], g[is_dis],\n","                        m[is_dis], f_L[is_dis], dn, True)\n","      # Note: I assume penalty based on old value of n since it is never 0\n","      penalty = torch.minimum(m[is_dis], 1 / torch.abs(n_old) * m[is_dis])     \n","      m[is_dis] = m[is_dis] - penalty\n","      A_B[is_dis], A_S[is_dis] =self.maintain_pool_ratio(penalty, A_B[is_dis],\n","                                                         A_S[is_dis])\n","    # Withdraw excess from margin account: Steps 6 - 7\n","    b = torch.maximum(m - torch.abs(n)*self.ts*M*self.l_r, torch.zeros_like(n)) \n","    m = m - b\n","    g = g + b\n","    return A_B, A_S, n, g, m\n","\n","  def make_trade(self, A_B, A_S, n, g, m, f_L, dn, close_out=False):\n","    \"\"\"\n","    Make a long or short trade of size equal to dn * self.ts with necessary \n","    updates\n","    \"\"\"\n","    A_B, A_S, n, g, m = self.margin(A_B, A_S, n, g, m, f_L, dn, close_out)\n","    # No need to mark to market with only one trader\n","    A_B, A_S, n, g, m = self.margin_checks(A_B, A_S, n, g, m, f_L)\n","    return A_B, A_S, n, g, m\n","\n","  def fee_update(self, A_B, A_S, n, g, m, f_L):\n","      \"\"\"Placehold for state update for fees\"\"\"\n","      return f_L\n","\n","  def trade_step(self, A_B, A_S, n, g, m, f_L, dn, s_ref):\n","    \"\"\"\n","    This function is somewhat extrenuous but leave open an easy possibility of\n","    extending to dn of magnitude greater than 1\n","\n","    dn : int\n","      Integer number of units of size self.ts to trade\n","    s_ref : \n","      the price on the reference market that agents have knowledge of\n","    \"\"\"\n","\n","    if dn < 0:\n","      change_in_position = -dn\n","    else:\n","      change_in_position = dn\n","    # Save so that profit can be calculated\n","    g_old = g.clone()\n","    m_old = m.clone()\n","    # Adjust the position by trading the necessary number of units\n","    for i in range(0, change_in_position):\n","      A_B, A_S, n, g, m = self.make_trade(A_B, A_S, n, g, m, f_L, dn)   \n","    # Simulate other agents to adjust market\n","    if not self.random_agent_update:\n","      A_B, A_S, n, g, m =\\\n","        self.oracle_settlement_price_update(s_ref, A_B, A_S, n, g, m, f_L)\n","    else:\n","      for i in range(self.num_traders):\n","        A_B, A_S, n, g, m = self.simulate_trader(s_ref, A_B, A_S, n, g, m, f_L)\n","    profit = (g+m) - (g_old+m_old)\n","    f_L = self.fee_update(A_B, A_S, n, g, m, f_L)\n","    return A_B, A_S, n, g, m, f_L, profit\n","\n","  def oracle_settlement_price_update(self, P, A_B, A_S, n, g, m, f_L):\n","    ''' \n","    Update pools A_B, A_S to match price P according to 2.2 in the paper \n","    \"Constant Product Market for Cash Settled Futures\"\n","    '''\n","    \n","    P_old = A_B / A_S\n","    deltaP = (A_B-P*A_S) / (1+P)\n","    A_B = A_B - deltaP\n","    A_S = A_S + deltaP\n","    A_B, A_S, n, g, m = self.mark_to_market(P_old, A_B, A_S, n, g, m)\n","    A_B, A_S, n, g, m = self.margin_checks(A_B, A_S, n, g, m, f_L)   \n","    return  A_B, A_S, n, g, m\n","\n","  def simulated_trade(self, A_B, A_S, dn, f_L):\n","    \"\"\"Simulate a trade of dn by a trader agent\"\"\"\n","\n","    # Ensure simulated trade will not cause pool to become distressed\n","    can_trade = self.check_pool_price_update(A_B, A_S, dn)\n","    # Adjust the pool position based on the trade\n","    A_B[can_trade], A_S[can_trade] =\\\n","      self.pool_price_update(A_B[can_trade], A_S[can_trade], dn[can_trade])\n","    # Take fee from simulated trader\n","    if self.traders_pay_fees:\n","      F_L = torch.abs(dn) * self.ts * A_B / A_S * f_L\n","      A_B[can_trade], A_S[can_trade] =\\\n","        self.maintain_pool_ratio(F_L[can_trade], A_B[can_trade],\n","                                  A_S[can_trade])\n","    return A_B, A_S\n","\n","  def simulate_trader(self, s, A_B, A_S, n, g, m, f_L):\n","      \"\"\"\n","      Simulate a trader on the market by according to which market offers\n","      the best price and trading with 50% probability\n","      \"\"\"\n","\n","      P_old = A_B / A_S\n","      # Determine what trade to make\n","      if self.traders_pay_fees:\n","        zeros = torch.zeros_like(s, dtype=torch.float64, device=self.device)\n","        ones = torch.ones_like(s, dtype=torch.float64, device=self.device)\n","        dn = torch.where(\\\n","          P_old + self.ts*f_L*P_old < s, ones, \n","          torch.where(P_old - self.ts*f_L*P_old > s, -ones, zeros)) \\\n","            * torch.where(torch.randn(A_B.size(0), device=self.device) < 0., 0.,\n","                          1.)\n","      else:\n","        dn = torch.where(P_old < s, 1., -1.) \\\n","          * torch.where(torch.randn(A_B.size(0), device=self.device) < 0., 0.,\n","                        1.)\n","      # Execute trade according to the usual steps, marking to market for\n","      # the trained agent\n","      A_B, A_S = self.simulated_trade(A_B, A_S, dn, f_L)\n","      A_B, A_S, n, g, m = self.mark_to_market(P_old, A_B, A_S, n, g, m)\n","      A_B, A_S, n, g, m = self.margin_checks(A_B, A_S, n, g, m, f_L)\n","      return A_B, A_S, n, g, m\n","\n","  def C(self, x):\n","      \"\"\" As the unimplemented policy the agent always does nothing \"\"\"\n","      \n","      c = torch.zeros((x.size(0),3), device=self.device) \n","      c[:,2] = torch.ones_like(c[:,2])\n","      return c\n","\n","  def step(self, x: torch.Tensor, train = False, s_next = None, **kwargs):\n","    \"\"\"\n","    Do a state update step in the CPFM and reference market based on the policy\n","    of the agent\n","\n","    Parameters\n","    ----------\n","    x: torch.Tensor\n","      State\n","    kwargs: dict\n","      arguments necessary to calculate a step of mid-price \n","    s_next: torch.Tensor\n","      manually provide the next state of the reference market. This is useful\n","      when evaluating policites against each other to provide each agent with\n","      the same market conditions\n","    \"\"\"\n","\n","    c = self.C(x)\n","    # Extract the state variables\n","    s, pool_price, pool_sum, n, g, m =\\\n","      x[:,0], x[:,1], x[:,2], x[:,3], x[:,4], x[:,5]\n","    # Extract the fees variable in variable fees case\n","    if self.variable_fees:\n","      f_L = x[:,6]\n","    else: \n","      f_L = self.f_L * torch.ones_like(s, device=self.device)\n","    # Derive pool values for calculations\n","    A_S = pool_sum / (pool_price + 1)\n","    A_B = pool_price * A_S \n","    # If not training, select an action from the policy distribution otherwise\n","    # use the probabilities to get an expected next state\n","    if not train:\n","      q = Categorical(c)\n","      c = one_hot(q.sample(), num_classes = 3)\n","    # Either update the reference market price by a GBM or take the value given\n","    if s_next is None:\n","      s_step = partial(GBM_step, **kwargs)\n","      s_next = s_step(s=s)\n","    else:\n","      s_next = s_next      \n","    # Long position on futures\n","    A_B_long, A_S_long, n_long, g_long, m_long, f_long, profit_long = \\\n","      self.trade_step(torch.clone(A_B), torch.clone(A_S), torch.clone(n),\n","                      torch.clone(g), torch.clone(m), torch.clone(f_L), 1, s)\n","    # Short position on futures\n","    A_B_short, A_S_short, n_short, g_short, m_short, f_short, profit_short = \\\n","      self.trade_step(torch.clone(A_B), torch.clone(A_S), torch.clone(n),\n","                      torch.clone(g), torch.clone(m), torch.clone(f_L), -1, s)\n","    # Do nothing\n","    A_B_not, A_S_not, n_not, g_not, m_not, f_not, profit_not = \\\n","      self.trade_step(torch.clone(A_B), torch.clone(A_S), torch.clone(n),\n","                      torch.clone(g), torch.clone(m), torch.clone(f_L), 0, s)\n","    # Get the expected value of each state variable\n","    n = c[:,0]*n_long + c[:,1]*n_short + c[:,2]*n_not\n","    g = c[:,0]*g_long + c[:,1]*g_short + c[:,2]*g_not\n","    m = c[:,0]*m_long + c[:,1]*m_short + c[:,2]*m_not\n","    f_L = c[:,0]*f_long + c[:,1]*f_short + c[:,2]*f_not\n","    # Ensure that the price and the total pool balance are equal to their \n","    # expectations, rather than the specific balances of A_B and A_S\n","    price = c[:,0]*A_B_long/A_S_long + c[:,1]*A_B_short/A_S_short \\\n","      + c[:,2]*A_B_not/A_S_not\n","    pool_balance = c[:,0]*(A_B_long+A_S_long) + c[:,1]*(A_B_short+A_S_short) \\\n","      + c[:,2]*(A_B_not+A_S_not)\n","    A_S = pool_balance / (price+1)\n","    A_B = pool_balance - A_S\n","    # Calculate expected profit\n","    profit = c[:,0]*profit_long + c[:,1]*profit_short + c[:,2]*profit_not\n","    # Recombine the state varaibles into one tensor  \n","    if self.variable_fees:\n","      x_next = torch.stack((s_next, price, pool_balance, n, g, m, f_L), dim=1)\n","    else:\n","      x_next = torch.stack((s_next, price, pool_balance, n, g, m), dim=1)\n","    # Calculate the running reward / utility\n","    running_cost = self.f(x_next, profit).reshape(-1,1)\n","    return c, x_next, running_cost, profit"]},{"cell_type":"code","source":["class RandomAgent(CPFMAgent):\n","  \"\"\"A CPFMAgent that selects a random action at each time-step\"\"\"\n","    \n","  def C(self, x):      \n","      m = nn.Softmax(dim=1)\n","      c = torch.randn((x.size(0),3), device=self.device) \n","      return m(c)"],"metadata":{"id":"n7fOSF4WClLA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IdealOneStepAgent(CPFMAgent):\n","  \"\"\"\n","  A CPFMAgent that will choose the ideal policy for a market following the\n","  exact price match update model\n","  \"\"\"\n","  \n","  def C(self, x):\n","    # Extract the state variables\n","    s, price, sum, n, g, m = x[:,0], x[:,1], x[:,2], x[:,3], x[:,4], x[:,5]\n","    # Extract fees in variable fees case\n","    if self.variable_fees:\n","      f_L = x[:,6]\n","    else:\n","      f_L = self.f_L\n","    A_S = sum / (1 + price)\n","    A_B = price * A_S\n","    c = torch.zeros((x.size(0),3), device=self.device)\n","    # Calculate the one-step profit from each action\n","    profit_long = (n + 1) * (s - (A_B + A_B / A_S) / (A_S - A_B / A_S))\\\n","      - A_B / A_S * (self.f_I + f_L)\n","    profit_short = (n - 1) * (s - (A_B - A_B / A_S) / (A_S + A_B / A_S))\\\n","      - A_B / A_S * (self.f_I + f_L)\n","    profit_not = n * (s - A_B / A_S)\n","    # Set the action with the highest one step profit\n","    c[:, 0] = torch.where((profit_long > profit_short)\\\n","                          & (profit_long > profit_not), 1., 0.)\n","    c[:, 1] = torch.where((profit_short > profit_long)\\\n","                          & (profit_short > profit_not), 1., 0.)\n","    c[:, 2] = torch.where((c[:, 0] == 0.) & (c[:, 1] == 0.), 1., 0.)\n","    return c"],"metadata":{"id":"0hIcu6ERKGto"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IdealOneStepAgentTraders(CPFMAgent):\n","  \"\"\"\n","  A CPFMAgent that will choose the ideal policy for a market following the\n","  simulated random trader price update model\n","  \"\"\"\n","\n","  def Pr(self, s, A_B, A_S, n, dn, k, f_L):\n","    \"\"\"Calculate the expected profit if exactly k random agents make a trade\"\"\"\n","    F_L = A_B / A_S * f_L\n","    A_B_prime = A_B + self.ts * dn * A_B / A_S\n","    A_S_prime = A_S - self.ts * dn * A_B / A_S \n","    A_B_prime, A_S_prime = self.maintain_pool_ratio(F_L, A_B_prime, A_S_prime)\n","    P_prime = A_B_prime / A_S_prime\n","    expectation = torch.zeros_like(s)\n","    for i in range(k):\n","      zeros = torch.zeros_like(s, dtype=torch.float64)\n","      ones = torch.ones_like(s, dtype=torch.float64)\n","      if self.traders_pay_fees:\n","        F_L = A_B_prime / A_S_prime * f_L\n","      else:\n","        F_L = torch.zeros_like(A_B_prime)\n","      dn_prime = torch.where(P_prime + F_L < s, ones,\n","                             torch.where(P_prime - F_L > s,-ones, zeros))\n","      F_L = torch.where(torch.abs(dn_prime) > 0., F_L, 0.)\n","      A_B_new = (A_B_prime + self.ts * dn_prime * P_prime)\n","      A_S_new = (A_S_prime - self.ts * dn_prime * P_prime)\n","      A_B_new, A_S_new = self.maintain_pool_ratio(F_L, A_B_new, A_S_new)\n","      expectation += (n + dn) * self.ts * (A_B_new / A_S_new - P_prime)\n","      A_B_prime = A_B_new.clone()\n","      A_S_prime = A_S_new.clone()\n","      P_prime = A_B_prime  / A_S_prime\n","    return expectation\n","\n","  def C(self, x):\n","    s, price, sum, n, g, m = x[:,0], x[:,1], x[:,2], x[:,3], x[:,4], x[:,5]\n","    if self.variable_fees:\n","      f_L = x[:,6]\n","    else:\n","      f_L = self.f_L * torch.ones_like(x[:,0])\n","    A_S = sum / (1 + price)\n","    A_B = price * A_S\n","    c = torch.zeros((x.size(0),3), device =self.device)\n","    expected_long_profit = torch.zeros_like(s, dtype=torch.float64)\n","    expected_short_profit = torch.zeros_like(s, dtype=torch.float64)\n","    expected_not_profit = torch.zeros_like(s, dtype=torch.float64)\n","    # Assume self.num_traders set\n","    for k in range(self.num_traders + 1):\n","      expected_long_profit += 0.5**self.num_traders\\\n","        * scipy.special.binom(self.num_traders, k)\\\n","        * self.Pr(s, A_B, A_S, n, 1., k, f_L)\n","      expected_short_profit += 0.5**self.num_traders\\\n","        * scipy.special.binom(self.num_traders, k)\\\n","        * self.Pr(s, A_B, A_S, n, -1., k, f_L)\n","      expected_not_profit += 0.5**self.num_traders\\\n","        * scipy.special.binom(self.num_traders, k)\\\n","        * self.Pr(s, A_B, A_S, n, 0., k, f_L)\n","\n","    expected_long_profit -= A_B / A_S * (self.f_I + f_L)\n","    expected_short_profit -= A_B / A_S * (self.f_I + f_L)\n","\n","    c[:, 0] = torch.where((expected_long_profit > expected_short_profit)\\\n","                          & (expected_long_profit > expected_not_profit),\n","                          1., 0.)\n","    c[:, 1] = torch.where((expected_short_profit > expected_long_profit)\\\n","                          & (expected_short_profit > expected_not_profit),\n","                          1., 0.)\n","    c[:, 2] = torch.where((c[:, 0] == 0.) & (c[:, 1] == 0.), 1., 0.)\n","    return c"],"metadata":{"id":"5eJppl8aSPUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_activation_from_str(activation_str):\n","\n","  if activation_str == 'nn.Identity':\n","    return nn.Identity\n","  elif activation_str == 'nn.Softmax':\n","    return nn.Softmax\n","  elif activation_str == 'nn.ReLU':\n","    return nn.ReLU\n","  elif activation_str == 'nn.Sigmoid':\n","    return nn.Sigmoid\n","  else:\n","    raise ValueError('Unknown activation string')"],"metadata":{"id":"HhPsw6JiIY8f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://towardsdatascience.com/understanding-pytorch-activation-functions-the-maths-and-algorithms-part-2-1f8bce111a7b"],"metadata":{"id":"5izKKU58kepT"}},{"cell_type":"code","source":["import math\n","from torch.autograd.grad_mode import F\n","class ActorCritic(CPFMAgent):\n","  \"\"\"\n","  A CPFMAgent trained via an actor critic algorithm to approximate the \n","  value function and optimal policy function\n","  \"\"\"\n","\n","  def __init__(self, utility : partial, trade_size : float, \n","               fee_infrastructure: float, R_long: float, R_short: float,\n","               l_search: float, l_initial: float, l_release: float,\n","               random_agent_update : bool, variable_fees : bool, \n","               CPFM_kw_args : dict, x0_sampler : partial, \n","               discount_factor: float, dims: int, verbose = False,\n","               device = 'cpu', gamma = 0.1, lr = 0.005, **kwargs):\n","    \"\"\"  \n","    Parameters\n","    ----------\n","    a: float\n","      Utility function parameter\n","    trade_size: float\n","      minimun trade size on constant product market   \n","    discount_factor: float\n","        Discount factor in Bellman equation between (0,1)\n","    device: str\n","        Device where things are run\n","    tau: float\n","        Change of time, in time discretisation\n","    sigma: float\n","        Diffusion in LQR SDE. I assume the diffusion is constant. \n","        Can be easily changed\n","    \"\"\"\n","    \n","    super().__init__(utility, trade_size, fee_infrastructure, \n","                      R_long, R_short, l_search, l_initial, l_release,\n","                      random_agent_update, variable_fees, CPFM_kw_args, \n","                      verbose = verbose, device = device, **kwargs)\n","    \n","    self.x0_sampler = x0_sampler\n","    self.discount_factor = discount_factor\n","    self.d = dims # dimension of the state (s, A_B, A_S, n, g, m)\n","  \n","    self.C = FFN(sizes = [self.d] + kwargs['hidden_dims_C'] + [3],\n","                  activation=get_activation_from_str(kwargs['activation_C']),\n","                  output_activation=\\\n","                  get_activation_from_str(kwargs['output_activation_C']), \n","                  normalize_input=kwargs['normalize_input_C'],\n","                  normalize_hidden=kwargs['normalize_hidden_C'],\n","                  normalize_output=kwargs['normalize_output_C']).to(self.device) \n","\n","    init_weights_C = partial(init_weights, var = kwargs['initial_weight_var_C'])\n","    self.C.apply(init_weights_C)\n","    \n","    # Optimize\n","    self.optimizer_C = torch.optim.Adam(self.C.parameters(), lr=lr)\n","\n","    if 'scheduler' in kwargs.keys() and kwargs['scheduler'] == 'Exponential':\n","      # Decay the learning rate\n","      self.scheduler_C = \\\n","        torch.optim.lr_scheduler.ExponentialLR(self.optimizer_C, gamma)\n","    else:\n","      # Decay the learning rate\n","      self.scheduler_C = \\\n","        torch.optim.lr_scheduler.MultiStepLR(self.optimizer_C,\n","                                              milestones=kwargs['milestones'],\n","                                              gamma=gamma)\n","    \n","    # value function, input of v is x\n","    self.v = FFN(sizes = [self.d] + kwargs['hidden_dims_v'] + [1],\n","                 activation=get_activation_from_str(kwargs['activation_v']),\n","                 output_activation=\\\n","                    get_activation_from_str(kwargs['output_activation_v']), \n","                 normalize_input=kwargs['normalize_input_v'],\n","                 normalize_hidden=kwargs['normalize_hidden_v'],\n","                 normalize_output=kwargs['normalize_output_v']).to(self.device)\n","\n","    init_weights_v = partial(init_weights, var = kwargs['initial_weight_var_v'])\n","    self.v.apply(init_weights_v)\n","\n","    self.optimizer_v = torch.optim.Adam(self.v.parameters(), lr=lr)\n","\n","    if 'scheduler' in kwargs.keys() and kwargs['scheduler'] == 'Exponential':\n","      # Decay the learning rate\n","      self.scheduler_v = \\\n","        torch.optim.lr_scheduler.ExponentialLR(self.optimizer_v, gamma)\n","    else:\n","      self.scheduler_v = \\\n","        torch.optim.lr_scheduler.MultiStepLR(self.optimizer_v,\n","                                             milestones=kwargs['milestones'],\n","                                             gamma=gamma)\n","\n","  def step(self, x, n_mc, train = True, s_next = None, **kwargs):\n","      \"\"\" \n","      Override the default step function to have training be True by default\n","      and to take the parameter n_mc for Monte-Carlo sampling \n","      \"\"\"\n","      n_batch = x.shape[0]\n","      x_mc = torch.repeat_interleave(x, n_mc, dim=0)\n","      return super().step(x_mc, train = train, s_next = s_next, **kwargs)\n","\n","  \n","  def _dynamic_programming(self, x: torch.Tensor, n_mc: int, **kwargs):\n","      \"\"\"\n","      Performs one step environment step and return bellman loss\n","      \n","      Parameters\n","      ----------\n","      x: torch.Tensor\n","          tensor. tensor of shape (N_batch, 3)\n","      n_mc: int\n","          Number of monte carlo samples to approximate drift\n","      kwargs: dict\n","          arguments necessary to calculate a step of mid-price \n","      \n","      Returns\n","      ------\n","      bellman_loss: torch.Tensor\n","          bellman loss: ( v(x) - 1/N_mc \\sum(f + delta * v(x_next)) )^2\n","      \n","      bellman_approx: torch.Tensor\n","          bellman approximation of v(x): 1/N_mc \\sum(f + delta * v(x_next))\n","      \n","      \"\"\"\n","      n_batch = x.shape[0]\n","      _, x_next, running_cost, _ = self.step(x, n_mc, train=True, **kwargs) \n","      # bellman loss (equation (5) in paper)\n","      bellman_approx = running_cost + self.discount_factor * self.v(x_next) \n","      # Average over Monte-Carlo Samples (equation (5))\n","      bellman_approx = bellman_approx.reshape(n_batch, n_mc, -1).mean(1)  \n","      # Average over batches and take power of 2 (equation (5))\n","      bellman_loss = torch.pow(self.v(x) - bellman_approx.detach(),2).mean()\n","      return bellman_loss, bellman_approx.mean()\n","  \n","  def update_alpha(self, n_batch, n_mc, **kwargs):\n","      \"\"\"\n","      Gradient ascent on alpha to maximise bellman approx\n","\n","      Parameters\n","      ----------\n","      n_batch: int\n","          batch size\n","      n_mc: int\n","          Monte Carlo size for Monte Carlo approximation of running cost to \n","          have some exploration\n","      kwargs: dict\n","          arguments necessary to calculate a step of mid-price \n","      \"\"\"\n","      toggle(self.v, to=False)\n","      toggle(self.C, to=True)\n","      self.C.train()\n","      x0 = self.x0_sampler(n_batch) \n","      self.optimizer_C.zero_grad()\n","      _, bellman_approx = self._dynamic_programming(x0, n_mc, **kwargs)\n","      bellman_approx = -1. * bellman_approx # we want to maximise!\n","      bellman_approx.backward()\n","      self.optimizer_C.step()\n","      self.scheduler_C.step()\n","      return -bellman_approx.detach()\n","  \n","  def update_v(self, n_batch, n_mc, **kwargs):\n","      \"\"\"\n","      Gradient descent on to minimise bellman loss\n","      \n","      Parameters\n","      ----------\n","      n_batch: int\n","          batch size\n","      n_mc: int\n","          Monte Carlo size for Monte Carlo approximation of running cost to \n","          have some exploration\n","      kwargs: dict\n","          arguments necessary to calculate a step of mid-price \n","      \"\"\"\n","      toggle(self.v, to=True)\n","      toggle(self.C, to=False)\n","      self.v.train()\n","      x0 = self.x0_sampler(n_batch)\n","      self.optimizer_v.zero_grad()\n","      bellman_loss, _ = self._dynamic_programming(x0, n_mc, **kwargs)\n","      bellman_loss.backward()\n","      self.optimizer_v.step()\n","      self.scheduler_v.step()\n","      return bellman_loss.detach()"],"metadata":{"id":"5RbPCmAGCusJ"},"execution_count":null,"outputs":[]}]}